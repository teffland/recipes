{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recipe_link_to_dict(link):\n",
    "    \"\"\" Take in the link to a NYTimes Cooking recipe \n",
    "        and extract all of the necessary info in dict form\n",
    "        This includes:\n",
    "         * name\n",
    "         * description\n",
    "         * prep time\n",
    "         * yield\n",
    "         * a url for the photo\n",
    "         * all of the tags (formerly categories)\n",
    "         * all of the ingredients, broken down into name, quantity, unit, and comment\n",
    "         * all of the steps: number and instruction text\n",
    "         NOTE: We do not pull user comments and names because it doesn't come with the original html, it\n",
    "         s jscript loaded when the user clicks 'show comments'\n",
    "    \"\"\"\n",
    "    soup = bs(requests.get(link).content, 'html.parser')\n",
    "    recipe = {}\n",
    "    recipe['name'] = soup.find('h1', class_='recipe-title').text\n",
    "    recipe['description'] = soup.find('div', class_='topnote').p.text\n",
    "    recipe['preparation_time'] = soup.find('ul', class_=\"recipe-time-yield\").li.text.replace('Time','').strip()\n",
    "    recipe['yield'] = soup.find('span', itemprop=\"recipeYield\").text.strip()\n",
    "    recipe['photo_url'] = soup.find('img', itemprop='image')['src']\n",
    "\n",
    "    # get all the tags that work\n",
    "    recipe['tags'] = []\n",
    "    for a in soup.find('p', class_=\"special-diets tag-block\").contents:\n",
    "        try:\n",
    "            recipe['tags'].append({'name': a.text})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # get ingredient info\n",
    "    recipe['ingredients'] = []\n",
    "    for li in soup.find_all('li', itemprop=\"recipeIngredient\"):\n",
    "        ingredient = {'quantity': li.find('span', class_='quantity').text}\n",
    "        name = li.find('span', itemprop='name').text\n",
    "        ingredient['name'] = name\n",
    "        # unit is section of phrase BEFORE name, comment is section AFTER\n",
    "        split = li.find('span', class_='ingredient-name').text.split(name,1) #maxsplit=1\n",
    "        ingredient['unit'] = split[0]\n",
    "        ingredient['comment'] = split[1]\n",
    "        recipe['ingredients'].append(ingredient)\n",
    "        \n",
    "    # get the step info\n",
    "    recipe['steps'] = []\n",
    "    for i, li in enumerate(soup.find('ol', itemprop='recipeInstructions').contents):\n",
    "        try:\n",
    "            recipe['steps'].append({'number':i, 'instructions':li.text})\n",
    "        except: # some elements aren't li and break on .text\n",
    "            pass\n",
    "    \n",
    "    return recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_json(data, fname='recipe_data.json'):\n",
    "    with open(fname, 'w') as fp:\n",
    "        json.dump(data, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root = 'http://cooking.nytimes.com'\n",
    "num_searches = 10\n",
    "search_links = [ root+'/search?q=&page='+str(n+1) for n in range(num_searches)]\n",
    "\n",
    "print \"Getting recipe links\"\n",
    "recipe_links = []\n",
    "for i, link in enumerate(search_links):\n",
    "    soup = bs(requests.get(link).content, 'html.parser')\n",
    "    new_recipe_links = [root+link['href'] for link in soup.find_all('a', class_=\"card-recipe-info\")]\n",
    "    recipe_links += new_recipe_links\n",
    "print 'Number of recipes links grabbed: %i' % len(recipe_links)\n",
    "\n",
    "# load each recipe link and convert it to a dict\n",
    "recipes = []\n",
    "for i, link in enumerate(recipe_links):\n",
    "    print \"Downloading recipe #%i\" % (i+1)\n",
    "    # it would appear a lot of these pages aren't as well-formed as I thought\n",
    "    # or (more likely) my code is buggy, but there's plenty of ones that work\n",
    "    # so just skip them gracefully\n",
    "    try:\n",
    "        recipes.append(recipe_link_to_dict(link))\n",
    "    except (NameError, TypeError, ValueError, AttributeError) as e:\n",
    "        print \"SKIPPED\", e\n",
    "    \n",
    "print \"Finished, %i total recipes successfully downloaded. Now writing out to file\" % len(recipes)\n",
    "write_json(recipes)\n",
    "print \"All Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
